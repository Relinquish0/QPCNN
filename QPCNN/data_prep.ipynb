{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8c8cbd",
   "metadata": {},
   "source": [
    "### Import necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f347cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install alphashape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ebf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'functions_lib/pt_generator.py'\n",
    "%run 'model.py'\n",
    "import numpy as np\n",
    "from functions_lib.pt_generator import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47933e",
   "metadata": {},
   "source": [
    "We note that now we set parameter $\\mu=1$. If you want to get three characterization metrics when $\\mu\\neq1$, file \"pt_generator\" provides adjustable parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7b309",
   "metadata": {},
   "source": [
    "### Generating 5 types of data:\n",
    "    1. pt_uncorrelated_train -- work as training set in pretraining phase\n",
    "    2. pt_uncorrelated_validation -- work as validation set in pretraining phase\n",
    "    3. pt_correlated_train -- work as training set in training phase\n",
    "    4. pt_correlated_validation -- work as validation set in training phase\n",
    "    5. pt_correlated_output -- work as output set in output phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated(n=100000,excel_path='/home/featurize/work/QPCNN/Dataset/pt_correlated_train.xlsx',mode=2)\n",
    "correlated(n=4000,excel_path='/home/featurize/work/QPCNN/Dataset/pt_correlated_validation.xlsx',mode=2)\n",
    "correlated(n=10000,excel_path='/home/featurize/work/QPCNN/Dataset/pt_correlated_output.xlsx',mode=5)\n",
    "uncorrelated(n=10000,excel_path='/home/featurize/work/QPCNN/Dataset/pt_uncorrelated_train.xlsx')\n",
    "uncorrelated(n=4000,excel_path='/home/featurize/work/QPCNN/Dataset/pt_uncorrelated_validation.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a0259",
   "metadata": {},
   "source": [
    "### Simulated experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count(N = 100000,input_file = '/home/featurize/work/QPCNN/Dataset/pt_correlated_train.xlsx',\n",
    "      output_file = '/home/featurize/work/QPCNN/Dataset/pm_correlated_train.xlsx',if_count = False)\n",
    "\n",
    "count(N = 100000,input_file = '/home/featurize/work/QPCNN/Dataset/pt_correlated_validation.xlsx',\n",
    "      output_file = '/home/featurize/work/QPCNN/Dataset/pm_correlated_validation.xlsx',if_count = False)\n",
    "\n",
    "count(N = 100000,input_file = '/home/featurize/work/QPCNN/Dataset/pt_correlated_output.xlsx',\n",
    "      output_file = '/home/featurize/work/QPCNN/Dataset/pm_correlated_output.xlsx',if_count = True)\n",
    "\n",
    "count(N = 100000,input_file = '/home/featurize/work/QPCNN/Dataset/pt_uncorrelated_train.xlsx',\n",
    "      output_file = '/home/featurize/work/QPCNN/Dataset/pm_uncorrelated_train.xlsx',if_count = False)\n",
    "\n",
    "count(N = 100000,input_file = '/home/featurize/work/QPCNN/Dataset/pt_uncorrelated_validation.xlsx',\n",
    "      output_file = '/home/featurize/work/QPCNN/Dataset/pm_uncorrelated_validation.xlsx',if_count = False)\n",
    "\n",
    "shuffle_excel('/home/featurize/work/QPCNN/Dataset/pm_correlated_train.xlsx')\n",
    "\n",
    "shuffle_excel('/home/featurize/work/QPCNN/Dataset/pm_uncorrelated_train.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f9309",
   "metadata": {},
   "source": [
    "### Pretraining phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_date, cur_hour, cur_min = get_time()\n",
    "hp_pretrain = HyperParameters_pretrain('/home/featurize/work/QPCNN',cur_date, cur_hour, cur_min)\n",
    "\n",
    "for folder1 in ['QPCNN_log', 'QPCNN_saved', 'QPCNN_output']:#log,saved,output.\n",
    "    os.makedirs('{}/output_Data/{}/'.format(hp_pretrain.save_path,folder1), exist_ok=True)#exist_ok = True means the existence of this file is OK \n",
    "    if folder1 == 'QPCNN_output':#Inference means output, Validation is not used in our work. \n",
    "        for folder2 in ['validation', 'inference']:\n",
    "            os.makedirs('{}/output_Data/{}/{}/'.format(hp_pretrain.save_path,folder1,folder2), exist_ok=True)\n",
    "\n",
    "# Log Setting \n",
    "logging.basicConfig(\n",
    "    filename=hp_pretrain.log_path,#Create a FileHandler with a file name called hp.log_path, that is, log information to the file hp.log_path\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',#Time - Level - Information\n",
    "    datefmt='%m-%d %H:%M',#Month - Date Hour: minute\n",
    "    level=logging.INFO#info\n",
    ")\n",
    "\n",
    "# Check whether the system can use a GPU for calculation. If no GPU is available, use a CPU\n",
    "hp_pretrain.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#在日志中记录训练的参数\n",
    "logging.info('Iteration times in this training: {}'.format(hp_pretrain.iteration))  # Record the number of iterations for this training\n",
    "logging.info('Computational Device: {}'.format(hp_pretrain.device))  # The processor (CPU or GPU) used in the calculation is recorded in the log\n",
    "logging.info('Learning parameters: lr-{}, lr_decay-{}, lr_patience-{}, early_patience-{}'.format(hp_pretrain.lr, hp_pretrain.lr_decay, hp_pretrain.lr_patience, hp_pretrain.early_patience))\n",
    "logging.info('#' * 50)\n",
    "\n",
    "# Model\n",
    "qpcnn = QPCNN()  # \n",
    "qpcnn.to(hp_pretrain.device) #CPU or GPU\n",
    "optimizer = optim.Adam(qpcnn.parameters(), lr=hp_pretrain.lr)  # Adam optimizer,\n",
    "# optimizer = optim.SGD(spircnet.parameters(), lr=1e-3, momentum=0.99)  # SGD optimizer\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=hp_pretrain.lr_decay, patience=hp_pretrain.lr_patience,\n",
    "                                                    verbose=True, threshold=hp_pretrain.threshold, min_lr=hp_pretrain.min_lr)  # learning rate scheduler\n",
    "# early_stopping = EarlyStopping(patience=hp.early_patience, delta=hp.min_delta, verbose=False, path=hp.model_save_path)\n",
    "\n",
    "since = time.time()  \n",
    "\n",
    "set_file(hp_pretrain.save_path) #Convert excel data in a file package to numpy for easy operation\n",
    "\n",
    "data_input_train = np.load(hp_pretrain.data_train)  # Loading training data\n",
    "input_vector_train, qubit_vector_train, qubit_spin_train, input_spin_train = data_prepartion_trainandtest(data_input_train)\n",
    "instance_train= qubit_vector_train.shape[0]\n",
    "\n",
    "data_input_test = np.load(hp_pretrain.data_test)  # Loading testing data\n",
    "input_vector_test, qubit_vector_test, qubit_spin_test,input_spin_test = data_prepartion_trainandtest(data_input_test)\n",
    "instance_test = input_vector_test.shape[0]\n",
    "\n",
    "qpcnn = torch.nn.DataParallel(qpcnn)    #Multi-GPU acceleration\n",
    "criterion_MSE_train = nn.MSELoss()  # Loss Function MAE(L1)\n",
    "criterion_MAE_train = nn.L1Loss()  # Loss Function MSE\n",
    "MSE_plot = []   # Draw an empty array and load MSE, MAE and Shannon entropy data\n",
    "MAE_plot = []   \n",
    "entropy_plot = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(hp_pretrain.iteration):\n",
    "\n",
    "    for each_instance_train in range(0,instance_train):\n",
    "        #============================================================================================\n",
    "        if each_instance_train % hp_pretrain.test_log == 0:\n",
    "            loss_test_summary_MAE = 0\n",
    "            loss_test_summary_MSE = 0\n",
    "            summary_entropy = 0\n",
    "            criterion_MAE_test = nn.L1Loss()\n",
    "            criterion_MSE_test = nn.MSELoss()\n",
    "            for each_instance_test in range(instance_test):\n",
    "                output_spin_test = qpcnn(input_vector_test[each_instance_test,:,:].to(hp_pretrain.device), qubit_vector_test[each_instance_test,:,:].to(hp_pretrain.device), qubit_spin_test[each_instance_test,:,:].to(hp_pretrain.device))\n",
    "                loss_test_MSE = criterion_MSE_test(output_spin_test[0,:,:].to(hp_pretrain.device), input_spin_test[each_instance_test,:,:].to(hp_pretrain.device)).data  \n",
    "                loss_test_summary_MSE += loss_test_MSE\n",
    "                loss_test_MAE = criterion_MAE_test(output_spin_test[0,:,:].to(hp_pretrain.device), input_spin_test[each_instance_test,:,:].to(hp_pretrain.device)).data  \n",
    "                loss_test_summary_MAE += loss_test_MAE\n",
    "            loss_test_summary_MSE = loss_test_summary_MSE/instance_test\n",
    "            loss_test_summary_MAE = loss_test_summary_MAE/instance_test\n",
    "            MSE_plot.append(loss_test_summary_MSE.cpu().numpy())#MSE\n",
    "            MAE_plot.append(loss_test_summary_MAE.cpu().numpy())#MAE\n",
    "            lr_scheduler.step(loss_test_summary_MSE)\n",
    "            #============================================================================================\n",
    "            logging.info('loss_test_MSE ={}'.format(loss_test_summary_MSE))\n",
    "            print('iter:{},epoch:{},validation number:{},MSE:{},MAE:{},entropy:{}'.format(iter,each_instance_train,instance_test,loss_test_summary_MSE,loss_test_summary_MAE,summary_entropy))   \n",
    "\n",
    "        qpcnn.train() \n",
    "        optimizer.zero_grad() \n",
    "        output_spin_train = qpcnn(input_vector_train[each_instance_train,:,:].to(hp_pretrain.device), qubit_vector_train[each_instance_train,:,:].to(hp_pretrain.device), qubit_spin_train[each_instance_train,:,:].to(hp_pretrain.device))\n",
    "        loss_train = criterion_MSE_train(output_spin_train[0,:,:].to(hp_pretrain.device), input_spin_train[each_instance_train,:,:].to(hp_pretrain.device)) \n",
    "        loss_train.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        if each_instance_train == instance_train-1:\n",
    "            loss_test_summary_MAE = 0\n",
    "            loss_test_summary_MSE = 0\n",
    "            summary_entropy = 0\n",
    "            criterion_MAE_test = nn.L1Loss()\n",
    "            criterion_MSE_test = nn.MSELoss()\n",
    "            for each_instance_test in range(instance_test):\n",
    "                output_spin_test = qpcnn(input_vector_test[each_instance_test,:,:].to(hp_pretrain.device), qubit_vector_test[each_instance_test,:,:].to(hp_pretrain.device), qubit_spin_test[each_instance_test,:,:].to(hp_pretrain.device))\n",
    "                loss_test_MSE = criterion_MSE_test(output_spin_test[0,:,:].to(hp_pretrain.device), input_spin_test[each_instance_test,:,:].to(hp_pretrain.device)).data\n",
    "                loss_test_summary_MSE += loss_test_MSE\n",
    "                loss_test_MAE = criterion_MAE_test(output_spin_test[0,:,:].to(hp_pretrain.device), input_spin_test[each_instance_test,:,:].to(hp_pretrain.device)).data  \n",
    "                loss_test_summary_MAE += loss_test_MAE\n",
    "            loss_test_summary_MSE = loss_test_summary_MSE/instance_test\n",
    "            loss_test_summary_MAE = loss_test_summary_MAE/instance_test\n",
    "            MSE_plot.append(loss_test_summary_MSE.cpu().numpy())#MSE\n",
    "            MAE_plot.append(loss_test_summary_MAE.cpu().numpy())#MAE           \n",
    "            lr_scheduler.step(loss_test_summary_MSE)  \n",
    "            print('iter:{},epoch:{},validation number:{},MSE:{},MAE:{},entropy:{}'.format(iter,each_instance_train,instance_test,loss_test_summary_MSE,loss_test_summary_MAE,summary_entropy))\n",
    "\n",
    "    if iter % hp_pretrain.train_log == 0:\n",
    "        print(\"---iter{} finished---\".format(iter))\n",
    "        logging.info('iter {}, train loss: {}'.format(iter, loss_train.data))\n",
    "        logging.info('*' * 50)\n",
    "if hp_pretrain.logging_parameter == True:\n",
    "    checkpoint = {'net': qpcnn.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "              'lr_scheduler': lr_scheduler.state_dict()}\n",
    "    torch.save(checkpoint, hp_pretrain.model_save_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cf933",
   "metadata": {},
   "source": [
    "### Training phase and output phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_date, cur_hour, cur_min = get_time()\n",
    "hp_output = HyperParameters_output('/home/featurize/work/QPCNN',cur_date, cur_hour, cur_min,'/home/featurize/work/QPCNN/output_Data/QPCNN_saved/2025-08-11-19h35min.pth')\n",
    "\n",
    "# Folder creation\n",
    "for folder1 in ['QPCNN_log', 'QPCNN_saved', 'QPCNN_output']:#log,saved,output.\n",
    "    os.makedirs('{}/output_Data/{}/'.format(hp_output.save_path,folder1), exist_ok=True)#exist_ok = True means the existence of this file is OK \n",
    "    if folder1 == 'QPCNN_output':#Inference means output, Validation is not used in our work. \n",
    "        for folder2 in ['validation', 'inference']:\n",
    "            os.makedirs('{}/output_Data/{}/{}/'.format(hp_output.save_path,folder1,folder2), exist_ok=True)\n",
    "\n",
    "# Log Setting \n",
    "logging.basicConfig(\n",
    "    filename=hp_output.log_path,#Create a FileHandler with a file name called hp.log_path, that is, log information to the file hp.log_path\n",
    "    filemode='a',\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',#Time - Level - Information\n",
    "    datefmt='%m-%d %H:%M',#Month - Date Hour: minute\n",
    "    level=logging.INFO#info\n",
    ")\n",
    "\n",
    "#================================================================================= importing pt and pm for calculating pn\n",
    "pm = pd.read_excel(\"{}/Dataset/pm_correlated_output.xlsx\".format(hp_output.save_path))\n",
    "pm = pm.to_numpy()\n",
    "pm_0 = pm[:,7]\n",
    "pm_1 = pm[:,8]\n",
    "\n",
    "pt = pd.read_excel(\"{}/Dataset/pt_correlated_output.xlsx\".format(hp_output.save_path))\n",
    "pt = pt.to_numpy()\n",
    "pt_trans = np.zeros((2*pt.shape[0],2))\n",
    "for i in range(pt.shape[0]):\n",
    "    pt_trans[i,0] = pt[i,6]/(pt[i,6]+pt[i,7])\n",
    "    pt_trans[i,1] = pt[i,7]/(pt[i,6]+pt[i,7])\n",
    "    pt_trans[i+pt.shape[0],0] = pt[i,8]/(pt[i,8]+pt[i,9])\n",
    "    pt_trans[i+pt.shape[0],1] = pt[i,9]/(pt[i,8]+pt[i,9])\n",
    "pt_0 = pt_trans[:,0]\n",
    "pt_1 = pt_trans[:,1]\n",
    "#============================================================================================================\n",
    "\n",
    "# Check whether the system can use a GPU for calculation. If no GPU is available, use a CPU\n",
    "hp_output.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.info('Iteration times in this training: {}'.format(hp_output.iteration))  # Record the number of iterations for this training\n",
    "logging.info('Computational Device: {}'.format(hp_output.device))  # The processor (CPU or GPU) used in the calculation is recorded in the log\n",
    "logging.info('Learning parameters: lr-{}, lr_decay-{}, lr_patience-{}, early_patience-{}'.format(hp_output.lr, hp_output.lr_decay, hp_output.lr_patience, hp_output.early_patience))\n",
    "logging.info('#' * 50)\n",
    "\n",
    "# Model\n",
    "qpcnn = QPCNN()  # \n",
    "qpcnn.to(hp_output.device) #CPU or GPU\n",
    "optimizer = optim.Adam(qpcnn.parameters(), lr=hp_output.lr)  # Adam optimizer,\n",
    "# optimizer = optim.SGD(spircnet.parameters(), lr=1e-3, momentum=0.99)  # SGD optimizer\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=hp_output.lr_decay, patience=hp_output.lr_patience,\n",
    "                                                    verbose=True, threshold=hp_output.threshold, min_lr=hp_output.min_lr)  # learning rate scheduler\n",
    "early_stopping = EarlyStopping(patience=hp_output.early_patience, delta=hp_output.min_delta, verbose=False, path=hp_output.model_save_path)\n",
    "\n",
    "if hp_output.training_resume == True:  # inheriting the model\n",
    "    checkpoint = torch.load(hp_output.model_load_path)  # loading\n",
    "    qpcnn = torch.nn.DataParallel(qpcnn).cuda() #Multi-card parallel nn.DataParallel data previously reserved for multi-card parallel data\n",
    "    qpcnn.load_state_dict(checkpoint['net'])  \n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])  \n",
    "    #lr_scheduler.load_state_dict(checkpoint['lr_scheduler']) \n",
    "    logging.info('Load and continue training the existing model: {}'.format(hp_output.model_load_path))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = hp_output.lr #Go back to the original learning rate\n",
    "else:  \n",
    "    logging.info('New created model')\n",
    "#============================================================================================================\n",
    "\n",
    "since = time.time()  \n",
    "set_file(hp_output.save_path)#Convert excel data in a file package to numpy for easy operation\n",
    "data_input_train = np.load(hp_output.data_train)  # Loading training data\n",
    "input_vector_train, qubit_vector_train, qubit_spin_train, input_spin_train = data_prepartion_trainandtest(data_input_train) # 处理训练集数据\n",
    "instance_train= qubit_vector_train.shape[0]\n",
    "\n",
    "data_input_test = np.load(hp_output.data_test)  # Loading testing data\n",
    "input_vector_test, qubit_vector_test, qubit_spin_test,input_spin_test = data_prepartion_trainandtest(data_input_test) # 处理训练集数据\n",
    "instance_test = input_vector_test.shape[0]\n",
    "\n",
    "qpcnn = torch.nn.DataParallel(qpcnn)    #Multi-GPU acceleration\n",
    "criterion_MSE_train = nn.MSELoss()  # Loss Function MAE(L1)\n",
    "criterion_MAE_train = nn.L1Loss()  # Loss Function MSE\n",
    "MSE_plot = []   # Draw an empty array and load MSE, MAE and Shannon entropy data\n",
    "MAE_plot = []   \n",
    "entropy_plot = [] \n",
    "epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ca6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(hp_output.iteration):\n",
    "\n",
    "    for each_instance_train in range(0,instance_train):# batch = 1\n",
    "        #=============================================Calculate MSE, MAE before each training===============================================\n",
    "        if each_instance_train % hp_output.test_log == 0:\n",
    "            loss_test_summary_MAE = 0\n",
    "            loss_test_summary_MSE = 0\n",
    "            summary_entropy = 0\n",
    "            criterion_MAE_test = nn.L1Loss()\n",
    "            criterion_MSE_test = nn.MSELoss()\n",
    "            for each_instance_test in range(instance_test):\n",
    "                output_spin_test = qpcnn(input_vector_test[each_instance_test,:,:].to(hp_output.device), qubit_vector_test[each_instance_test,:,:].to(hp_output.device), qubit_spin_test[each_instance_test,:,:].to(hp_output.device))\n",
    "                loss_test_MSE = criterion_MSE_test(output_spin_test[0,:,:].to(hp_output.device), input_spin_test[each_instance_test,:,:].to(hp_output.device)).data  \n",
    "                loss_test_summary_MSE += loss_test_MSE\n",
    "                loss_test_MAE = criterion_MAE_test(output_spin_test[0,:,:].to(hp_output.device), input_spin_test[each_instance_test,:,:].to(hp_output.device)).data  \n",
    "                loss_test_summary_MAE += loss_test_MAE\n",
    "            loss_test_summary_MSE = loss_test_summary_MSE/instance_test\n",
    "            loss_test_summary_MAE = loss_test_summary_MAE/instance_test\n",
    "            MSE_plot.append(loss_test_summary_MSE.cpu().numpy())#MSE\n",
    "            MAE_plot.append(loss_test_summary_MAE.cpu().numpy())#MAE\n",
    "            lr_scheduler.step(loss_test_summary_MSE)  # Based on loss_test_summary_MAE to adjust Learning rate\n",
    "            logging.info('loss_test_MSE ={}'.format(loss_test_summary_MSE))\n",
    "            print('loss_test_MSE ={},loss_test_MAE ={}'.format(loss_test_summary_MSE,loss_test_summary_MAE))\n",
    "\n",
    "            \n",
    "            #=============================================output phase:===============================================\n",
    "            data_input_infer = np.load(hp_output.data_infer)  \n",
    "            data_input_infer = data_input_infer[:,:7]\n",
    "            input_vector_infer, qubit_vector_infer, qubit_spin_infer = data_prepartion_infer(data_input_infer) \n",
    "            instance_infer = input_vector_infer.shape[0]\n",
    "            output_summary_infer = np.zeros((instance_infer,2))\n",
    "            for each_instance_infer in range(instance_infer):\n",
    "                output_spin_infer = qpcnn(input_vector_infer[each_instance_infer,:,:].to(hp_output.device), qubit_vector_infer[each_instance_infer,:,:].to(hp_output.device), qubit_spin_infer[each_instance_infer,:,:].to(hp_output.device)) #shape(1,1,8)\n",
    "                output_spin_infer = output_spin_infer.view(1,2)\n",
    "                output_spin_infer = output_spin_infer.detach().cpu().numpy() \n",
    "                \n",
    "                po_0 = output_spin_infer[0,1]*pt_0[each_instance_infer]*pm_0[each_instance_infer] / (output_spin_infer[0,0]*pt_1[each_instance_infer]*pm_1[each_instance_infer] + output_spin_infer[0,1]*pt_0[each_instance_infer]*pm_0[each_instance_infer])\n",
    "                po_0_clipped = np.clip(po_0, 1e-10, 1 - 1e-10)# ensure p != 0\n",
    "                summary_entropy = summary_entropy - po_0_clipped * np.log(po_0_clipped) - (1 - po_0_clipped) * np.log(1 - po_0_clipped)\n",
    "                # output_spin_infer[0,0] = po_0_clipped\n",
    "                # output_spin_infer[0,1] = 1 - po_0_clipped\n",
    "                output_summary_infer[each_instance_infer,:] = output_spin_infer # now we output pn, if you want to output po(n->infinite), use the last two lines of code\n",
    "\n",
    "            summary_entropy = summary_entropy / instance_infer\n",
    "            entropy_plot.append(summary_entropy)#entropy\n",
    "            #print(output_spin_infer.shape)  #torch.Size([1, 1, 2])\n",
    "            print('iter:{},epoch:{},validation number:{},MSE:{},MAE:{},entropy:{}'.format(iter,each_instance_train,instance_test,loss_test_summary_MSE,loss_test_summary_MAE,summary_entropy))\n",
    "            logging.info('iter:{},epoch:{},validation number:{},MSE:{},MAE:{},entropy:{}'.format(iter,each_instance_train,instance_test,loss_test_summary_MSE,loss_test_summary_MAE,summary_entropy))\n",
    "\n",
    "            output_summary_infer = np.concatenate((data_input_infer[:,:7],output_summary_infer),axis=1)\n",
    "            np.save(hp_output.output_save_path + '/inference/output.npy', output_summary_infer)  # save\n",
    "            colname = ['input_a1_x','input_a1_y','input_a1_z',\n",
    "                                                    'qubit_a1_x','qubit_a1_y','qubit_a1_z',\n",
    "                                                    'qubit_a1_spin',\n",
    "                                                    '0','1']\n",
    "            \n",
    "            pd.DataFrame(output_summary_infer,columns = colname).to_excel(hp_output.output_save_path + '/inference/output_iter{}_epoch{}.xlsx'.format(iter,epoch),index=True,\n",
    "                                        columns = colname)\n",
    "            epoch = epoch+1\n",
    "            print('iter:{},epoch:{},output number:{}'.format(iter,each_instance_train,instance_infer))    \n",
    "\n",
    "        qpcnn.train() \n",
    "        optimizer.zero_grad() # grading resetting\n",
    "        output_spin_train = qpcnn(input_vector_train[each_instance_train,:,:].to(hp_output.device), qubit_vector_train[each_instance_train,:,:].to(hp_output.device), qubit_spin_train[each_instance_train,:,:].to(hp_output.device))\n",
    "        loss_train = criterion_MSE_train(output_spin_train[0,:,:].to(hp_output.device), input_spin_train[each_instance_train,:,:].to(hp_output.device)) \n",
    "        loss_train.backward()  # (Calculate the gradient value of the parameter) by backpropagation\n",
    "        optimizer.step()  # Update parameters (by gradient descent)\n",
    "\n",
    "        if each_instance_train == instance_train-1:#last\n",
    "            #sfwml.eval()    \n",
    "            loss_test_summary_MAE = 0\n",
    "            loss_test_summary_MSE = 0\n",
    "            summary_entropy = 0\n",
    "            criterion_MAE_test = nn.L1Loss()\n",
    "            criterion_MSE_test = nn.MSELoss()\n",
    "            for each_instance_test in range(instance_test):\n",
    "                output_spin_test = qpcnn(input_vector_test[each_instance_test,:,:].to(hp_output.device), qubit_vector_test[each_instance_test,:,:].to(hp_output.device), qubit_spin_test[each_instance_test,:,:].to(hp_output.device))\n",
    "                loss_test_MSE = criterion_MSE_test(output_spin_test[0,:,:].to(hp_output.device), input_spin_test[each_instance_test,:,:].to(hp_output.device)).data  \n",
    "                loss_test_summary_MSE += loss_test_MSE\n",
    "                loss_test_MAE = criterion_MAE_test(output_spin_test[0,:,:].to(hp_output.device), input_spin_test[each_instance_test,:,:].to(hp_output.device)).data \n",
    "                loss_test_summary_MAE += loss_test_MAE\n",
    "            loss_test_summary_MSE = loss_test_summary_MSE/instance_test\n",
    "            loss_test_summary_MAE = loss_test_summary_MAE/instance_test\n",
    "            MSE_plot.append(loss_test_summary_MSE.cpu().numpy())#MSE\n",
    "            MAE_plot.append(loss_test_summary_MAE.cpu().numpy())#MAE\n",
    "            lr_scheduler.step(loss_test_summary_MSE)  \n",
    "            \n",
    "            #=============================================The same as previous work===============================================\n",
    "            data_input_infer = np.load(hp_output.data_infer)  # \n",
    "            input_vector_infer, qubit_vector_infer, qubit_spin_infer = data_prepartion_infer(data_input_infer)\n",
    "            instance_infer = input_vector_infer.shape[0]\n",
    "            output_summary_infer = np.zeros((instance_infer,2))\n",
    "            for each_instance_infer in range(instance_infer):\n",
    "                output_spin_infer = qpcnn(input_vector_infer[each_instance_infer,:,:].to(hp_output.device), qubit_vector_infer[each_instance_infer,:,:].to(hp_output.device), qubit_spin_infer[each_instance_infer,:,:].to(hp_output.device)) #shape(1,1,8)\n",
    "                output_spin_infer = output_spin_infer.view(1,2)\n",
    "                output_spin_infer = output_spin_infer.detach().cpu().numpy() \n",
    "                \n",
    "                po_0 = output_spin_infer[0,1]*pt_0[each_instance_infer]*pm_0[each_instance_infer] / (output_spin_infer[0,0]*pt_1[each_instance_infer]*pm_1[each_instance_infer] + output_spin_infer[0,1]*pt_0[each_instance_infer]*pm_0[each_instance_infer])\n",
    "                po_0_clipped = np.clip(po_0, 1e-10, 1 - 1e-10)\n",
    "                summary_entropy = summary_entropy - po_0_clipped * np.log(po_0_clipped) - (1 - po_0_clipped) * np.log(1 - po_0_clipped)\n",
    "                # output_spin_infer[0,0] = po_0_clipped\n",
    "                # output_spin_infer[0,1] = 1 - po_0_clipped\n",
    "                output_summary_infer[each_instance_infer,:] = output_spin_infer\n",
    "\n",
    "            summary_entropy = summary_entropy / instance_infer\n",
    "            entropy_plot.append(summary_entropy)#entropy\n",
    "            #print(output_spin_infer.shape)  #torch.Size([1, 1, 2])\n",
    "            print('iter:{},epoch:{},validation number:{},MSE:{},MAE:{},entropy:{}'.format(iter,each_instance_train,instance_test,loss_test_summary_MSE,loss_test_summary_MAE,summary_entropy))\n",
    "            output_summary_infer = np.concatenate((data_input_infer[:,:7],output_summary_infer),axis=1)\n",
    "            np.save(hp_output.output_save_path + '/inference/output.npy', output_summary_infer)  \n",
    "            colname = ['input_a1_x','input_a1_y','input_a1_z',\n",
    "                                                    'qubit_a1_x','qubit_a1_y','qubit_a1_z',\n",
    "                                                    'qubit_a1_spin',\n",
    "                                                    '0','1']\n",
    "            \n",
    "            pd.DataFrame(output_summary_infer,columns = colname).to_excel(hp_output.output_save_path + '/inference/output_iter{}_epoch{}.xlsx'.format(iter,epoch),index=True,\n",
    "                                        columns = colname)\n",
    "            print('iter:{},epoch:{},output number:{}'.format(iter,each_instance_train,instance_infer))\n",
    "\n",
    "    if iter % hp_output.train_log == 0:# loss_train is recorded every hp.train_log iter\n",
    "        print(\"---iter{} finished---\".format(iter))\n",
    "        logging.info('iter {}, train loss: {}'.format(iter, loss_train.data))\n",
    "        logging.info('*' * 50)\n",
    "#=================================================saving the model====================================================\n",
    "if hp_output.logging_parameter == True:\n",
    "    checkpoint = {'net': qpcnn.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "              'lr_scheduler': lr_scheduler.state_dict()}\n",
    "    torch.save(checkpoint, hp_output.model_save_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b28c5",
   "metadata": {},
   "source": [
    "### Generatoring $f$, $p^o$ and Shannon entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==2.3.2 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27431690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'functions_lib/exposeddata_generator.py'\n",
    "from functions_lib.exposeddata_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe318800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calculating_f('/home/featurize/work/QPCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculating_p_o('/home/featurize/work/QPCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf6fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculating_entropy('/home/featurize/work/QPCNN') \n",
    "for i in [0,1,2,5,20,100,101]: # We note 101 as unfrabricated data\n",
    "    polytopes('/home/featurize/work/QPCNN',i)\n",
    "    roundness_cal('/home/featurize/work/QPCNN', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0671a41",
   "metadata": {},
   "source": [
    "### PDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,5,20,100,101]:\n",
    "    real_experiment(file_path = '/home/featurize/work/QPCNN', times = i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
